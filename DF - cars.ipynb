{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed whenever working with spark dataframes\n",
    "from pyspark.sql import *\n",
    "!rm -rf metastore_db/\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using inferschema is very handy, but adds a second pass over the data\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .option('inferschema', 'true')\\\n",
    "                .option('mode', 'DROPMALFORMED')\\\n",
    "                .load('f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv')\n",
    "#look at Spark Web UI to see the two passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('car', 'mpg').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.filter(df['mpg'] < 18).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the filter can be just like a SQL where clause\n",
    "df.filter('mpg < 18').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataframes are immutable -- withColumn creates a new one\n",
    "df.withColumn('wtTon', df['wt'] * 0.45).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['cyl'])\\\n",
    ".agg({\"wt\": \"AVG\"})\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_counts = df.groupby(['cyl'])\\\n",
    ".agg({\"*\": \"count\"})\\\n",
    ".sort(\"count(1)\", ascending=False)\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"cars\")\n",
    "\n",
    "# SQL statements can be run by using the sql method\n",
    "highgearcars = sqlContext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\n",
    "highgearcars.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Step 1: Create a list of tuples (\"john\", 23), (\"mr. bean\", 56), (\"bill clinton\", 70)\n",
    "\n",
    "Step 2: Convert the list to a data frame with two columns\n",
    "\n",
    "Step 3: Filter the people with age < 40\n",
    "\n",
    "Step 4: Get the names ONLY of people smaller than 40\n",
    "\n",
    "Step 5: Register the dataframe as a SQL table\n",
    "\n",
    "Step 6: Select the names only of people whose age < 40\n",
    "\n",
    "Some hints: http://spark.apache.org/docs/latest/sql-programming-guide.html#generic-loadsave-functions\n",
    "\n",
    "## Exercise 2:\n",
    "\n",
    "Step 1: Create a list of tuple (\"john\", \"male\"), (\"hilary\", \"female\")\n",
    "\n",
    "Step 2: Join with the data from the previos exercise and output all males older than 50\n",
    "\n",
    "Hints: https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
